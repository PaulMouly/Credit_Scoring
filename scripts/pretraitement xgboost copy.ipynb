{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.02 GiB for an array with shape (445, 307511) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Chargement des données\u001b[39;00m\n\u001b[0;32m     15\u001b[0m chemin_dossier \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/paulm/Documents/Projet 7/Projet7withCSV/data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(chemin_dossier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Vérifier si SK_ID_CURR est présente et correcte avant prétraitement\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSK_ID_CURR\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1966\u001b[0m         new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n\u001b[1;32m-> 1968\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m   1969\u001b[0m         new_col_dict,\n\u001b[0;32m   1970\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   1971\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   1972\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write(),\n\u001b[0;32m   1973\u001b[0m     )\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_block_manager_from_column_arrays(\n\u001b[0;32m    153\u001b[0m         arrays, axes, consolidate\u001b[38;5;241m=\u001b[39mconsolidate, refs\u001b[38;5;241m=\u001b[39mrefs\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2144\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2142\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2144\u001b[0m     mgr\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2270\u001b[0m         \u001b[38;5;28mlist\u001b[39m(group_blocks), dtype\u001b[38;5;241m=\u001b[39mdtype, can_consolidate\u001b[38;5;241m=\u001b[39m_can_consolidate\n\u001b[0;32m   2271\u001b[0m     )\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2298\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2300\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2301\u001b[0m new_values \u001b[38;5;241m=\u001b[39m new_values[argsort]\n\u001b[0;32m   2302\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2304\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.02 GiB for an array with shape (445, 307511) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Chargement des données\n",
    "chemin_dossier = \"C:/Users/paulm/Documents/Projet 7/Projet7withCSV/data/\"\n",
    "df = pd.read_csv(os.path.join(chemin_dossier, 'processed_data.csv'))\n",
    "\n",
    "# Vérifier si SK_ID_CURR est présente et correcte avant prétraitement\n",
    "if 'SK_ID_CURR' in df.columns:\n",
    "    print(\"SK_ID_CURR est présente avant prétraitement.\")\n",
    "    print(df['SK_ID_CURR'].head())\n",
    "else:\n",
    "    raise ValueError(\"SK_ID_CURR n'est pas présente dans les données d'origine.\")\n",
    "\n",
    "# Extraire SK_ID_CURR pour une utilisation ultérieure\n",
    "sk_id_curr = df['SK_ID_CURR']\n",
    "\n",
    "# Sélection des colonnes catégorielles\n",
    "colonnes_categorielles = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Affichage du nombre de valeurs uniques dans chaque colonne catégorielle\n",
    "for col in colonnes_categorielles:\n",
    "    nb_valeurs_uniques = df[col].nunique()\n",
    "    print(f\"Colonne '{col}' contient {nb_valeurs_uniques} valeurs uniques.\")\n",
    "\n",
    "# Suppression des colonnes avec trop de valeurs uniques\n",
    "colonnes_a_supprimer = ['ORGANIZATION_TYPE', 'OCCUPATION_TYPE']\n",
    "df.drop(columns=colonnes_a_supprimer, inplace=True)\n",
    "\n",
    "# Identifier les colonnes numériques\n",
    "colonnes_numeriques = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Identifier les colonnes contenant des valeurs infinies\n",
    "colonnes_infinies = colonnes_numeriques.columns.to_series()[np.isinf(colonnes_numeriques).any()]\n",
    "\n",
    "# Affichage des colonnes avec des valeurs infinies\n",
    "print(\"Colonnes avec des valeurs infinies :\")\n",
    "print(colonnes_infinies)\n",
    "\n",
    "# Suppression des lignes contenant des valeurs infinies\n",
    "lignes_avec_infinis = df.index[np.isinf(colonnes_numeriques).any(axis=1)]\n",
    "df.drop(lignes_avec_infinis, inplace=True)\n",
    "\n",
    "# Vérification\n",
    "print(\"\\nReste-t-il des valeurs infinies ?\")\n",
    "reste_infinis = np.isinf(colonnes_numeriques).any().any()\n",
    "print(reste_infinis)\n",
    "\n",
    "# Si des valeurs infinies restent, les remplacer par NaN\n",
    "if reste_infinis:\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(\"\\nLes valeurs infinies ont été remplacées par NaN.\")\n",
    "\n",
    "# Retirer SK_ID_CURR avant encodage\n",
    "df.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "\n",
    "# Pipeline de prétraitement\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('drop_columns', 'drop'),  # Ajouter étape de suppression de colonnes\n",
    "    ('inf_to_nan', 'inf'),  # Ajouter étape de gestion des infinies\n",
    "    ('encoder', OrdinalEncoder()),  # Encodage des variables catégorielles\n",
    "    ('imputer', SimpleImputer(strategy='mean'))  # Imputation des valeurs manquantes\n",
    "])\n",
    "\n",
    "# Préparer les données\n",
    "X = df.drop(columns=['TARGET'])\n",
    "y = df['TARGET']\n",
    "\n",
    "# Appliquer le prétraitement\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Ajouter SK_ID_CURR de nouveau après encodage\n",
    "df_encoded = pd.DataFrame(X_preprocessed, columns=X.columns)\n",
    "df_encoded['SK_ID_CURR'] = sk_id_curr.values\n",
    "\n",
    "# Vérifier si SK_ID_CURR est présente et correcte après encodage\n",
    "if 'SK_ID_CURR' in df_encoded.columns:\n",
    "    print(\"SK_ID_CURR est présente après encodage.\")\n",
    "    print(df_encoded['SK_ID_CURR'].head())\n",
    "else:\n",
    "    raise ValueError(\"SK_ID_CURR n'est pas présente après encodage.\")\n",
    "\n",
    "# Séparation des données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vérifier si SK_ID_CURR est présente et correcte après séparation\n",
    "if 'SK_ID_CURR' in X_train.columns and 'SK_ID_CURR' in X_test.columns:\n",
    "    print(\"SK_ID_CURR est présente après séparation.\")\n",
    "    print(X_train['SK_ID_CURR'].head())\n",
    "    print(X_test['SK_ID_CURR'].head())\n",
    "else:\n",
    "    raise ValueError(\"SK_ID_CURR n'est pas présente après séparation.\")\n",
    "\n",
    "# Adresse de MLflow\n",
    "mlflow_url = \"http://127.0.0.1:5000\"\n",
    "\n",
    "# Coût d'un faux positif et d'un faux négatif\n",
    "cost_fp = 1  \n",
    "cost_fn = 10  \n",
    "\n",
    "# Définir le modèle XGBoost avec ses paramètres\n",
    "model = XGBClassifier(scale_pos_weight=10, random_state=42)\n",
    "params = {'n_estimators': [50, 100], 'max_depth': [3, 6, 9]}\n",
    "\n",
    "# Configurer l'expérience MLflow\n",
    "mlflow.set_tracking_uri(mlflow_url)\n",
    "mlflow.set_experiment(\"Credit Scoring Experiment\")\n",
    "\n",
    "# Échantillonner les données\n",
    "sample_size = 0.1 \n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=(1 - sample_size), random_state=42)\n",
    "X_test_sample, _, y_test_sample, _ = train_test_split(X_test, y_test, test_size=(1 - sample_size), random_state=42)\n",
    "\n",
    "# Entraînement du modèle XGBoost obtention des meilleurs paramètres\n",
    "with mlflow.start_run(run_name=\"XGBoost\"):\n",
    "    print(\"Entraînement du modèle XGBoost...\")\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, scoring='f1', cv=3)\n",
    "    grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "    # Loguer les paramètres dans MLflow\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Affichage des meilleurs paramètres et le score F1\n",
    "    print(f\"Meilleurs paramètres pour XGBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Score F1 moyen sur le jeu de validation: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "    # Utilisation du meilleur modèle pour les prédictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_sample)\n",
    "\n",
    "    # Évaluation du modèle avec le meilleur seuil pour minimiser le coût métier\n",
    "    y_prob = best_model.predict_proba(X_test_sample)[:, 1]\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresholded = (y_prob > threshold).astype(int)\n",
    "        fp = np.sum((y_pred_thresholded == 1) & (y_test_sample == 0)) * cost_fp\n",
    "        fn = np.sum((y_pred_thresholded == 0) & (y_test_sample == 1)) * cost_fn\n",
    "        total_cost = fp + fn\n",
    "        costs.append(total_cost)\n",
    "\n",
    "    best_threshold = thresholds[np.argmin(costs)]\n",
    "    print(f\"Meilleur seuil pour minimiser le coût métier avec XGBoost: {best_threshold}\")\n",
    "\n",
    "    y_pred_best_threshold = (y_prob > best_threshold).astype(int)\n",
    "\n",
    "    # Affichage du rapport de classification et de la matrice de confusion\n",
    "    classification_report_str = classification_report(y_test_sample, y_pred_best_threshold)\n",
    "    confusion_matrix_str = confusion_matrix(y_test_sample, y_pred_best_threshold)\n",
    "\n",
    "    print(f\"Rapport de classification avec le meilleur seuil pour XGBoost:\")\n",
    "    print(classification_report_str)\n",
    "\n",
    "    print(f\"Matrice de confusion avec le meilleur seuil pour XGBoost:\")\n",
    "    print(confusion_matrix_str)\n",
    "    print()\n",
    "\n",
    "    # Loguer les métriques et les résultats dans MLflow\n",
    "    mlflow.log_metric(\"best_score_f1\", grid_search.best_score_)\n",
    "    mlflow.log_metric(\"best_threshold\", best_threshold)\n",
    "    mlflow.log_text(classification_report_str, \"classification_report.txt\")\n",
    "    mlflow.log_text(str(confusion_matrix_str), \"confusion_matrix.txt\")\n",
    "\n",
    "    # Loguer le modèle\n",
    "    model_path = 'model/xgboost_model.pkl'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "    # Sauvegarder le prétraitement dans un fichier .pkl\n",
    "    preprocessing_pipeline = {\n",
    "        'drop_columns': colonnes_a_supprimer,\n",
    "        'inf_to_nan': lignes_avec_infinis,\n",
    "        'encoder': preprocessor.named_steps['encoder'],\n",
    "        'imputer': preprocessor.named_steps['imputer']\n",
    "    }\n",
    "\n",
    "    preprocessing_path = 'preprocessing/preprocessing_pipeline.pkl'\n",
    "    os.makedirs(os.path.dirname(preprocessing_path), exist_ok=True)\n",
    "    joblib.dump(preprocessing_pipeline, preprocessing_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              0.0\n",
      "1              1.0\n",
      "2              2.0\n",
      "3              3.0\n",
      "4              4.0\n",
      "            ...   \n",
      "307487    307487.0\n",
      "307488    307488.0\n",
      "307489    307489.0\n",
      "307490    307490.0\n",
      "307491    307491.0\n",
      "Name: SK_ID_CURR, Length: 307492, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_encoded['SK_ID_CURR'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
