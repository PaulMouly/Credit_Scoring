{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "chemin_dossier = \"C:/Users/paulm/Documents/Projet 7/Projet7withCSV/data/\" \n",
    "df = pd.read_csv(os.path.join(chemin_dossier, 'processed_data_encoded.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adresse de MLflow\n",
    "mlflow_url = \"http://127.0.0.1:5000\"\n",
    "\n",
    "# Coût d'un faux positif et d'un faux négatif\n",
    "cost_fp = 1  \n",
    "cost_fn = 10  \n",
    "\n",
    "# Gestion des valeurs manquantes avec SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(df.drop(columns=['TARGET'])), columns=df.drop(columns=['TARGET']).columns)\n",
    "\n",
    "# Séparation des caractéristiques et la cible\n",
    "y = df['TARGET']\n",
    "\n",
    "# Séparation des données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle XGBoost avec ses paramètres\n",
    "model = XGBClassifier(scale_pos_weight=10, random_state=42)  \n",
    "params = {'n_estimators': [50, 100], 'max_depth': [3, 6, 9]}  \n",
    "\n",
    "# Configurer l'expérience MLflow\n",
    "mlflow.set_tracking_uri(mlflow_url)\n",
    "mlflow.set_experiment(\"Credit Scoring Experiment\")\n",
    "\n",
    "# Échantillonner les données\n",
    "sample_size = 0.1 \n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, test_size=(1 - sample_size), random_state=42)\n",
    "X_test_sample, _, y_test_sample, _ = train_test_split(X_test, y_test, test_size=(1 - sample_size), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle XGBoost avec validation croisée...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:290: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype, copy=copy, casting=casting)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparams, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Entraînement avec GridSearchCV et suivi MLflow\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_sample, y_train_sample)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Loguer les meilleurs paramètres dans MLflow\u001b[39;00m\n\u001b[0;32m     26\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_params(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:879\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    875\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    877\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n\u001b[1;32m--> 879\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    880\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[0;32m    882\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2525\u001b[0m, in \u001b[0;36mcheck_cv\u001b[1;34m(cv, y, classifier)\u001b[0m\n\u001b[0;32m   2520\u001b[0m cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cv\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cv, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[0;32m   2522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2523\u001b[0m         classifier\n\u001b[0;32m   2524\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2525\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   2526\u001b[0m     ):\n\u001b[0;32m   2527\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m StratifiedKFold(cv)\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:389\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    387\u001b[0m     data \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39many(data \u001b[38;5;241m!=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(data, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m--> 389\u001b[0m         _assert_all_finite(data, input_name\u001b[38;5;241m=\u001b[39minput_name)\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m suffix\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Check multiclass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    127\u001b[0m     X,\n\u001b[0;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    133\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "# Adresse de MLflow\n",
    "mlflow_url = \"http://127.0.0.1:5000\"\n",
    "\n",
    "# Configurer l'expérience MLflow\n",
    "mlflow.set_tracking_uri(mlflow_url)\n",
    "mlflow.set_experiment(\"Credit Scoring Experiment\")\n",
    "\n",
    "# Définition du modèle XGBoost et des paramètres à optimiser\n",
    "model = XGBClassifier()\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Entraînement du modèle XGBoost avec validation croisée\n",
    "with mlflow.start_run(run_name=\"XGBoost with Cross-Validation\"):\n",
    "    print(\"Entraînement du modèle XGBoost avec validation croisée...\")\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=params, scoring='f1', cv=3)\n",
    "    \n",
    "    # Entraînement avec GridSearchCV et suivi MLflow\n",
    "    grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "    # Loguer les meilleurs paramètres dans MLflow\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    # Affichage des meilleurs paramètres et le score F1\n",
    "    print(f\"Meilleurs paramètres pour XGBoost: {grid_search.best_params_}\")\n",
    "    print(f\"Score F1 moyen sur le jeu de validation: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "    # Utilisation du meilleur modèle pour les prédictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Prédictions avec validation croisée pour obtenir les prédictions stables\n",
    "    y_pred = cross_val_predict(best_model, X_test_sample, y_test_sample, cv=3)\n",
    "\n",
    "    # Évaluation du modèle avec le meilleur seuil pour minimiser le coût métier\n",
    "    y_prob = best_model.predict_proba(X_test_sample)[:, 1]\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresholded = (y_prob > threshold).astype(int)\n",
    "        fp = np.sum((y_pred_thresholded == 1) & (y_test_sample == 0)) * cost_fp\n",
    "        fn = np.sum((y_pred_thresholded == 0) & (y_test_sample == 1)) * cost_fn\n",
    "        total_cost = fp + fn\n",
    "        costs.append(total_cost)\n",
    "\n",
    "    best_threshold = thresholds[np.argmin(costs)]\n",
    "    print(f\"Meilleur seuil pour minimiser le coût métier avec XGBoost: {best_threshold}\")\n",
    "\n",
    "    y_pred_best_threshold = (y_prob > best_threshold).astype(int)\n",
    "\n",
    "    # Affichage du rapport de classification et de la matrice de confusion\n",
    "    classification_report_str = classification_report(y_test_sample, y_pred_best_threshold)\n",
    "    confusion_matrix_str = confusion_matrix(y_test_sample, y_pred_best_threshold)\n",
    "\n",
    "    print(f\"Rapport de classification avec le meilleur seuil pour XGBoost:\")\n",
    "    print(classification_report_str)\n",
    "\n",
    "    print(f\"Matrice de confusion avec le meilleur seuil pour XGBoost:\")\n",
    "    print(confusion_matrix_str)\n",
    "    print()\n",
    "\n",
    "    # Loguer les métriques et les résultats dans MLflow\n",
    "    mlflow.log_metric(\"best_score_f1\", grid_search.best_score_)\n",
    "    mlflow.log_metric(\"best_threshold\", best_threshold)\n",
    "    mlflow.log_text(classification_report_str, \"classification_report.txt\")\n",
    "    mlflow.log_text(str(confusion_matrix_str), \"confusion_matrix.txt\")\n",
    "\n",
    "    # Loguer le modèle\n",
    "    model_path = 'model/xgboost_model.pkl'\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "    # Sauvegarder le modèle dans un fichier .pkl\n",
    "    joblib.dump(best_model, model_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
